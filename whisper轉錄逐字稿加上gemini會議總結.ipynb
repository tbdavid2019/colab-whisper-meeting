{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kT5aBdHllkyW",
        "outputId": "aadce0ff-9e95-4fc3-8748-336be0c0ff94",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-epvp0ycg\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-epvp0ycg\n",
            "  Resolved https://github.com/openai/whisper.git to commit 173ff7dd1d9fb1c4fddea0d41d704cfefeb8908c\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (4.66.6)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (10.5.0)\n",
            "Collecting tiktoken (from openai-whisper==20240930)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting triton>=2.0.0 (from openai-whisper==20240930)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper==20240930) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803557 sha256=4e96bdc5bada7855d07b46bf302bacd18cecb98c0f4739e4c97b09390c27a69c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6uweb6lo/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: triton, tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20240930 tiktoken-0.8.0 triton-3.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OEoNE5GUT7pW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from google.colab import files\n",
        "\n",
        "# 定義 summarize_with_gemini 函數\n",
        "def summarize_with_gemini(transcribed_text, api_key):\n",
        "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key={api_key}\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    payload = {\n",
        "        \"contents\": [\n",
        "            {\n",
        "                \"parts\": [\n",
        "                    {\"text\": f\"請用繁體中文輸出。Summarize the following meeting transcript into concise bullet points:\\n\\n{transcribed_text}\"}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        try:\n",
        "            # 修正解析邏輯，從 `candidates` 中提取摘要\n",
        "            summary = result[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "            return summary\n",
        "        except KeyError:\n",
        "            print(\"Unexpected response format:\", result)\n",
        "            return None\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code} - {response.text}\")\n",
        "        return None\n",
        "\n",
        "# 檢查文件是否存在\n",
        "audio_path = \"/content/water.m4a\"\n",
        "\n",
        "if not os.path.exists(audio_path):\n",
        "    print(\"檔案 water.m4a 不存在，請上傳音頻文件。\")\n",
        "    uploaded = files.upload()  # 上傳您的音頻檔案\n",
        "    audio_path = list(uploaded.keys())[0]  # 獲取上傳的檔案名稱\n",
        "\n",
        "# 加載 Whisper 模型\n",
        "model = whisper.load_model(\"medium\")\n",
        "result = model.transcribe(audio_path, language=\"zh\", task=\"transcribe\", initial_prompt=\"請轉錄以下繁體中文的內容：\")\n",
        "transcribed_text = result[\"text\"]\n",
        "\n",
        "# 打印轉錄結果\n",
        "print(\"Transcription:\")\n",
        "print(transcribed_text)\n",
        "\n",
        "# 使用 Gemini API 生成摘要\n",
        "api_key = \"AIzaSyAnX6P6YEJ-\"  # 替換為您的 Gemini API 密鑰\n",
        "summary = summarize_with_gemini(transcribed_text, api_key)\n",
        "\n",
        "# 輸出和保存結果\n",
        "if summary:\n",
        "    print(\"Meeting Summary:\")\n",
        "    print(summary)\n",
        "\n",
        "    # 保存轉錄和摘要為文本文件\n",
        "    file_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
        "    with open(f\"{file_name}.txt\", \"w\") as file:\n",
        "        file.write(transcribed_text)\n",
        "        files.download(f\"{file_name}.txt\")\n",
        "\n",
        "    with open(f\"{file_name}_summary.txt\", \"w\") as file:\n",
        "        file.write(summary)\n",
        "        files.download(f\"{file_name}_summary.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "844LWn9Bmb9H",
        "outputId": "6b216b62-bdb2-4483-a9b8-3fbfafb02f74",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription:\n",
            " Hello. Hi, hi, hi. Can you hear me? Yeah, hey, hi, how are you? Yeah, I can hear you. Okay, okay, great. I'm great. How are you? Yeah, I'm also doing good. Okay, so I'm just going to actually wait for the hiring manager to join us. Okay. Yeah. Yeah, and I think they'll probably ask you to introduce yourself, then we'll follow by coding. Like, we'll do an issue together. Well, you'll do a coding with them. You'll have to share your screen to show them what you're doing. You can explain while you are doing it or you can explain after it's up to you. And then afterwards, I think they'll leave and then I'll talk to you a little bit about our company and we'll see how we can co-work together. Yeah, sure. Okay, so I'll just wait a few seconds. Are you currently in India right now? Yes. So it's 1130 there, right? Correct, yes. Okay. This is the first time I actually know that time zone has like 30 minutes difference. I always thought it was an hour an hour. Okay, so you are at like which place in Vietnam? No, I'm in Taiwan. We're all based in Taiwan. We do have some developers across the globe. We have a few in Vietnam. We have a few in Canada. I think we have a few in Hong Kong as well. But the majority of the teams are actually in Taiwan. Okay, okay. I think we're like 70 in Taiwan at the moment. But well, since a lot of us actually work remotely, I have never been to the office myself. So I don't actually know how many are in the office, maybe like 20, 30. Okay. Yeah. Hold on a second. Let me message him. How do I pronounce your name right? Is it Sagar or Sagar? Oh, it's Sagar. I think the first one is correct. Yeah. Okay, okay, okay. Great. Okay. I think either David or Eric is in. Hi, Eric. Okay. I think we'll wait for David. Hi, Eric. Yeah, we're just waiting. Oh, he's here. David is here. Hi, David. Hello. Do you guys want to turn on your cam just to say hi or you're good? Up to you. David, do you want to turn on your cam? Yeah. Okay. Can we just turn on the cam for a second just to say hi? Okay. So, Sagar, very nice to meet you. Could you briefly introduce yourself to us? Yeah, sure. So, hi, all. I'm Sagar Shinde. I'm based out of India. I have a total of 10 years of experience working with Java and related technologies. So, it's not only Java that I do have experience. I work with like a node. I work with like a Python. On front-end side, it's React and Angular. React has been two years I'm working with. Before that, I used to work with Angular. So, I would say like I'm a backend heavy full stack. On database side, it's MySQL Postgres. Then NoSQL side, it's DynamoDB, MongoDB, both. For data warehousing, I work with Metabase and Clickhouse. Cloud side, it's AWS majorly, I would say. More than nine years I'm working with AWS. But I have experience with Azure as well as your JCP. So, yeah, last project I was working with like a North American client that is on E-commerce platform, restoration hardware. So, I was working there as a senior software engineer and working on automating the full security generation based on the colorization and all. There are different terminologies, actually, I would say, or different business logics, which we need to implement to generate a new product and their full skills, basically. So, yeah, I think that's pretty much from my side. Okay, Sagar, can you maybe show us your GitHub and maybe tell us one of two Java projects that you have recently been working on or something that you think that can represent you? Do you want to share me my screen and show you those? Yeah, that would be great. Okay, fine. So, I'm sharing my screen. So, I will say these are basically the POCs. The other projects I will show you that. Okay, so let's take an example of this patient care. And this is some POC I have built on. This is for one of the clients that I used to work with. So basically, this is like an I would say, and this is recently pushed, I would say like last month only I have pushed this code and I will show other code base as well, like which I am working on with the team, basically. So, this is what I was working on, which I do have. So, this is, there is a standard now, right, in USA, in Canada, they do have a standard, so store the data, healthcare data specifically, which is either generated by the EHR or open MRS or let's say any IoT devices, which are getting used in the healthcare sector. They need to define the data in the standard structure that is like an HL7, I would say like HL7 fire. So, they have designed this fire database, basically. Let's say if you want to store any sort of data, right, whether it's patient data or it's organization data or hospital data or let's say, take an example for, just so I can, fire patient. Okay, so basically, for patient, they do have a standard structure, they have defined. If you want to create a patient to save the number, to have the relationships, to store the contact, they have provided one standard global structure. So, whoever is using, like, having like a healthcare system which needs to be designed, they need to store this data in the fire to comply with the accompliances. So, that is what this HL7 standard brings into the picture. So, I got a task, basically. There is one client called like M-DASH, that is from USA, but development is happening in Pune itself, and they want like some POC and some guidance on that side. I used to work with that side. So, that is how it is worked. So, if you see this Edbox client, it was basically an open source tool that provides the underneath support for hosting the fire database. In this particular format. So, you can see like this is what this Edbox client, this is for create a patient, update the patient, get the patient by ID and delete the patient from the Edbox. And this is basically a rest controller for patient to create the patient, to search the patient based on that. Along with that, whenever there is something that is happening, like let's say a patient is getting created, they need to search that patient within the application. So, every time they don't want to call the fire database and they have like an elastic search at place. So, if you see like elastic search config is there. So, I have configured the elastic search. So, whenever there is a patient get created, right, so what we do, whenever we are creating the patient, we are updating that patient with the search service as well. So, that's what like the search service is in picture. So, if you go and implement, check the search service implementation, you will see like it is actually pushing that data to like this is less upsetting to the search service. And this is deleting from the search service whenever that delete is happening. And this is basically another thing like search by patient name and some other criteria, right. So, here it's just like based on the name right now that I'm searching on. So, this is a POC sort of project, I would say. There is another project that I was working with for one of the client that is from Canada, that is Cardizify. It's a digital card and digital catalog management system for them that we have designed. So, here it's containerize application. Okay, basically there are, this criteria is okay. Let's take digital cards basically. So, if user want to create a digital card for his business or his own account, so they can create digital cards with this. A particular account. I actually am not able to show you the service which I have designed for rh.com because I don't have like the GitHub account access right now. As that project got over. So, this is not on my private repo. It's on VPN connection based access. So, I was not able to do. But this is the site basically where I used to work generating, let's say, if you go like in any bid, let's say, fabric bids. So, every bids, yeah. So, if you see like this particular category have like an API, it's like a If you see like this particular category have like an if you see the rh fabric bids, it does have like a different skews all together based on like a different fabrics, different size, different length, width, or maybe different mattresses. So, category is same. The product is same, but there might be like a different skews for that. So, that's what like we have like an automated process which includes like Kafka messages. When our product gets created based on the input parameters, right, which we have used to create the catalog and define the colorization. We are creating a different skews for that. And every skew have like their own stock management. That depends on your geography. As this company is working in North America, in the UK, specifically, that is what their main demographic actually they are working with. So, yeah, I think that is what. So, another implementation is here we have used like a MongoDB to store the product and information, API information, pricing related information for that skew, as well as the color code for that skews, whatever is available, right, and those are standard color codes. So, yeah, I think this is pretty much. Do you want to know like any particular portion which I can go in depth and explain? Not an issue on that side. Sorry. Eric and David, you know,特別想要看其他什麼東西嗎?還是我直接請他做題目也可以。 有聽到嗎? 有有有。 看了該Arigo。 看了該Arigo。 看了該Arigo。 看了該Arigo。 看了該Arigo。 看了該Arigo。 看了該Arigo。 看了該Arigo。 看了該Arigo。 看了該Arigo。 看了該Arigo。 看了該Arigo。 看了該Arigo。 看了該Arigo。 看了該Arigo。 看了該Arigo。 看了該Arigo。 看了該Arigo。 看了該Arigo。 看了該Arigo。 OK. So, I'm going to give you some time to explain how you can do this while you share your screen with us. I will separate this into three little paragraphs because I can't post everything at once. Give me one second. And yeah, you can do it silently. You can explain after. You can do it while you explain however you like it. So, I'll give you some time and take your time. OK. Let me just check that. I will just bring this. I'll set up the application basically as this is required. These are the requirements, right? Correct or if odd? Did I miss something? Yeah. No, no. That's everything from the in-call messages. I think that's every... Hold on a second. Oh, did you use this as a... I think there is something more. You might have to scroll down. Yeah, that's everything. That's everything. That's everything I posted. This is everything, right? That's what I just want to... I will go through the statement first. I will go through the statement first. I will go through the statement first. I will go through the statement first. I will go through the statement first. I will go through the statement first. I will go through the statement first. I will go through the statement first. I will go through the statement first. I will go through the statement first. I will go through the statement first. I will go through the statement first. I will go through the statement first. I will go through the statement first. I will go through the statement first. I will go through the statement first. I will go through the statement first. I will go through the statement first. I will go through the statement first. I will go through the statement first. I will go through the statement first. I will go through the statement first. I will go through the statement first. What is this actually? There is anything like I need to do with this particular URL? No, just the first link is okay. Okay, fine. I just want to make sure. Okay. I need to do like a cache implementation basically. I would say like a third party integration, right? We have like this API which is giving us the, let's say, large number of data set, right? I'm just doing this. I can see you guys also. Okay. I got the problem statement that what I need to do, I will just repeat that just to make it more clear and make sure I'm on the right track. We need to build a sample application which will have an endpoint called API slash users. This user list is getting provided by this third party API call, right? Which we do have. For initial call, for get users, it will call to have a call with this particular endpoint, face the result, and it will save the data into the Redis cache. For other subsequent calls, whenever, let's say, second time, if I call API slash users, instead of calling this external API, it should directly get the data from Redis cache and return the list of customers, right? I think that is what I think I need to implement. Maybe I can start with that. If it's okay with you guys, like, should I start with this? You can start to, first you can start to write a REST API for the API slash users. Maybe you can print the hello world, the first step. And then you will try to write a service which will get data from the UI and you need to transfer it to the object, in the Java object, and then response to the REST API you write by the JSON format. And the third step is to set up the cache mechanism to cache the results. I think you can do it. What is the last part, Eric, that you said? The part is to use the cache. Maybe you can use the springs annotation. Yeah, I can use that. Okay, fine. Actually, this application is up and running. I don't think so. Hello is a need to have. So if you see, like, this is up and running right now. So I can start with the actual implementation for writing the endpoint. Okay. We need to enable the cache, right? Okay. Okay. Actually, I'm not sure what we need. Let me just build the API first. I will just implement the cache later. You can try to search anywhere, Google or chat.gbt, if you want. Okay, fine. Okay. Okay. Okay. Okay. Okay. Okay. The response that we need, right? We need like that in terms of like what? Okay. I will use this just like an object here. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Hello. Hi. Yeah. We need like a ready server, right? Without ready server, I cannot show like. Yeah, so I just mean that I think you don't need to actually use ready. You can just try to configure the cache configure using green boots default. And the default, it doesn't care about the underlying is it ready or something else. It doesn't care. So you can remove the red list. You can remove that is configuration and try again. Oh, OK. Got it. Got your point now. OK, sorry for my bad. I think that in a second, it's like an already. So that's what I heard. Eric, David, Sorry, we're running out of time. Do you want to ask him to explain? Maybe he hasn't finished yet. Wait a little bit more. Wait for him to finish this. OK, OK. OK. So if you see, I'm calling multiple times now. First time, it called like an external API. Now I'm calling this. It's not calling the API. Yeah, OK. It's putting the data from the right. OK, it works. Good. All the doors. So now they've been just trying to pull through. I just can't get out. So you might be able to get out of the way. How many of them? Mm, I'm going to be able to get out of the way. Maybe you have to have a way. OK, now, you may have a chance to kind of I'll tell him to explain. Oh, OK. Thank you, David. Thank you, Eric. No, leave us and I will I will explain the rest. OK, thank you. So thank you for the interview and thank you for doing that. And I would like to kind of just explain a little bit about our company. So as I said, we're based in Kaohsiung. And our employees can work remotely if you're not located anywhere near Kaohsiung. And since SAGAR is actually in India, I think if this is going to work out, we'll be it is a contracted job because we don't really have an entity in India. So we can't legally hire you. It's going to be a contract job. But we honestly prefer all of our employees to be in the long term base because it takes a lot of your time and also our hiring managers time to actually hire a suitable candidate. So basically all of our contracted jobs, except if they are not a good fit, otherwise we have never terminate a contracted employees. OK, awesome. Yeah, so this is our case. And you'll be paid by hour. And the salary can be discussed after I get the feedback from the hiring manager. So you'll be paid by hours. Every work day is eight hours, of course. Like the rest of the world, every work week is 40 hours. And if you need a day off, you can let the project manager know in advance, depends on which project you're working on. And even though we don't really have our own product, basically our major client is Wesson. So we're doing SAP e-commerce platform for them. And it's like a full stack development. So we do have our own front end. We do have our own back end. And recently, we have also been working on AI and ML. So we're working on recommendation systems for them. We're working on also computer vision, because we are working on augmented reality project, which we wanted to build an AR store in Hong Kong, basically, for customers to see this AR store front. So they can use the phone. Working on that, this is my project, basically. So I'm working on this AR project. And we're also recruiting ML engineers. So we're kind of expanding to different sectors-ish. Well, we still have a very weak ML at the moment, because we only have two. Well, I'm looking. So I'm still recruiting for my team. And I think we're going pretty good. I know we don't have our own product. But since we're taking over all of the Wesson's project, we are absolutely financially stable. And the only problem we have is we don't have enough people. Now we don't have enough projects. So we actually reject a lot of projects from them, because we just don't have enough people. And it's not, at the moment, very easy to recruit in IT, I think. Yeah. So this is our major problem. We are absolutely financially stable. We have a lot of things to do. We have a lot of visions. And yeah, we're just looking for the right fit at the moment. And if you could join us, well, I do think our salary is gross, though. So we don't pay your tax. So you don't have to calculate that. Yeah, I'm not sure about India law, if we are the one who is responsible for paying tax, or if you are the one who is responsible. Correct. If it's a contractor, then I need to do that. Yeah, so basically what we pay you is gross. You need to calculate it in that by yourself. Yeah, this is how it works. And also, your monthly salary might vary a little bit. It depends on the work days of the month, which you can possibly calculate the whole year if you know your hourly salary. No. Yeah. Work-wise, I honestly don't think we track our employees by hour, because sometimes I'm gone for a day and nobody is asking what I'm doing. So it's more likely how you manage yourself. But since I think you're very familiar with remote jobs, so I think you know how it works. And honestly, nobody has time to track you all the time. So you just do your own thing. Yeah, I think this is how it works. And other than that, I don't think I have anything special. And I'll get the feedback from the hiring manager. And it doesn't matter if it's a hire or not hire, I'll get you. I'll get back to you on LinkedIn. Or email. I do have your email. Yeah, yeah. There's one question I want to check with you. How long that I will, it's like, how soon that you are looking for this role? Because I also need to be. How long am I looking for this role? How soon that you are looking for, let's say. If we are hiring, then if it's a yes from us, then we could literally hire you the day after tomorrow or something, as long as we sign a contract and stuff. Yeah, because we are looking for eight engineers. So we're looking for a lot of people. So there's not a problem that we're not looking for people next year or something. We're looking for people now. So I think on boarding date is something that could be as soon as possible. OK, fine. Yeah, cool. Thank you. Thank you, Sagar. And I hope to hear from you soon. Yeah, I do too. Thank you. Bye.\n",
            "Meeting Summary:\n",
            "* **Interview Process:** The interview consisted of an introduction, a coding challenge (building a REST API with caching using Redis), and a company overview.\n",
            "\n",
            "* **Candidate (Sagar):**  A full-stack developer with 10 years of experience, proficient in Java, Python, Node.js, React, Angular, various databases (SQL and NoSQL), AWS, and Azure.  He showcased projects including a POC for HL7 FHIR standard implementation and a digital card management system.\n",
            "\n",
            "* **Coding Challenge:**  Sagar successfully implemented a REST API endpoint to retrieve user data, caching the results in Redis to optimize subsequent calls.\n",
            "\n",
            "* **Company (Based in Taiwan):** Primarily works with a major client (Wesson) on SAP e-commerce platform development, expanding into AI/ML and AR projects.  Mostly remote work; 70+ employees, many globally located.\n",
            "\n",
            "* **Employment Offer:** If successful, Sagar would be hired as a contractor due to the company's lack of an Indian entity.  Payment is hourly (8 hours/day, 40 hours/week), gross (no tax withholding), and potentially starts very soon.  The company has a strong need for engineers and many open positions.\n",
            "\n",
            "* **Next Steps:** The interviewer will provide feedback to Sagar via email or LinkedIn.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c521c08c-1176-42e0-b6d6-74c73d4362f0\", \"water.txt\", 20337)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_68cf4879-a7e2-4e27-8516-875e45c74e59\", \"water_summary.txt\", 0)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}